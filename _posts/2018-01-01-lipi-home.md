---
layout: post
title:  "Lipizzaner: A System That Scales Robust Generative Adversarial Network Training"
author: jamal
image: assets/images/alfa-logo.png
tags: [sticky]
---

Over the last few years, researchers and practitioners have found in **Generative Adversarial Networks** (GANs) a tool to address several challenging machine learning problems.
Most of these problems are related to generative machine learning, but we can find others like semi-supervised learning tacked to create classifiers by using few labeled data samples.
Despite its success, GAN training presents several limitiations or pathologies. The most commonly observed ones are:
+ *mode collapse*: the generator can only generate one mode of the distribution
+ *non-convergence*: the algorithm does not find an equilibrium, and
+ *vanishing gradients*: there are no training gradients, i.e. one adversary is too strong/weak.




At **[Anyscale Learning For All (ALFA)](http://alfagroup.csail.mit.edu/)**, we have designed and developed **Lipizzaner** framework, which applies **spatially distributed co-evolution** to provide **resilient and robust GAN training**.

The main advantages of using Lipizzaner to train GANs are:
+ **Fast convergence** due to gradient-based steps
+ **Improved convergence** due to hyperparameter evolution
+ **Diverse sample generation** due to mixture evolution
+ **Scalability** due to spatial distribution topology and asynchronous parallelism
+ **Robustness and resilience**

This website will include plenty of information allowing the users take advantage of this framework to deal with their own problems. 

If you have any coments, do not hesitate to contact us.
